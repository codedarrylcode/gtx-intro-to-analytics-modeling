{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regression\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "*Classification and regression trees*\n",
    "\n",
    "Suppose if a single covariate reflects a realistic situation where mixed effects are observed, for example, data points with age $\\leq 25$ and data points with age $> 25$, then a decision tree can fit a regression model and estimate its relevant coefficients in each set of data points, conditional on age.\n",
    "\n",
    "<img alt=\"CART\" src=\"assets/CART.png\" width=\"500\">\n",
    "\n",
    "This offers two direct benefits:\n",
    "\n",
    "1. Ability to examine each individual leaf's coefficients and explain the covariate's relationship with the target variable\n",
    "\n",
    "\n",
    "2. Higher predictive accuracy with targeted predictions\n",
    "\n",
    "****\n",
    "\n",
    "**Branching**\n",
    "\n",
    "*How does branching work?*\n",
    "\n",
    "In general, the idea is to do the following:\n",
    "\n",
    "- Use half of the data to begin branching\n",
    "- Find the best factor to branch\n",
    "    - Split the data on each factor\n",
    "    - Calculate the [mutual information](https://github.com/codedarrylcode/mitx-statistical-modeling/blob/master/notebooks/05a-gaussian-processes.ipynb) between factor and target\n",
    "    - Find the factor that has the largest mutual information\n",
    "- Accept branch if mutual information is more than threshold\n",
    "- Repeat until no further splits that are more than threshold\n",
    "- Use the other half of the data to prune the tree\n",
    "    - For each branch, calculate estimation error with/without the tree (*did this really improve the model?*)\n",
    "    - If branching increases error, then remove branching\n",
    "    \n",
    "\n",
    "<img alt=\"Branching\" src=\"assets/branching.png\" width=\"500\">\n",
    "\n",
    "In general, over-branching is likely to cause overfitting and a branch should be rejected if it doesn't cross the threshold, i.e. improvement benefit vs the cost of overfitting. A rule of thumb to prevent overfitting is to ensure that the leaf contains at least 5% of the original data\n",
    "\n",
    "****\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "The idea is to introduce randomness and generate many different trees such that the average of these trees outperforms a single tree. This procedure is called **bagging** (*bootstrap aggregating*) and also known as one of the [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) methods.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Bootstrapping $n$ data points with replacement from the original dataset\n",
    "\n",
    "2. Randomly choose a small subset of factors for branching, for e.g. $1 + \\log(n)$ # of factors\n",
    "\n",
    "3. No pruning necessary\n",
    "\n",
    "4. For regression, the average of all predictions are used and for classification, the most common predicted response is used (*averages also can be used as probability*)\n",
    "\n",
    "| Benefits      | Drawbacks |\n",
    "| ----------- | ----------- |\n",
    "| Better overall estimates      | Harder to explain/interpret results       |\n",
    "| Averages between trees and somewhat neutralizes over-fitting   | Can't give us a specific regression or classification model from the data        |\n",
    "\n",
    "****\n",
    "\n",
    "## Logistic Regression\n",
    "*Sometimes called a logit model*\n",
    "\n",
    "A standard linear regression model is as follows:\n",
    "\n",
    "$y = a_0 + a_1 x_1 + \\dots + a_j x_j = X^T \\alpha$ where $\\alpha$ is a vector of coefficients as estimated by MLE\n",
    "\n",
    "A logistic regression model transforms the above:\n",
    "\n",
    "$\\log (\\frac{p}{1-p}) = a_0 + a_1 x_1 + \\dots + a_j x_j = X^T \\alpha$\n",
    "\n",
    "$\\therefore p = \\frac{1}{1 + e^{-X^T \\alpha}}$\n",
    "\n",
    "This transformation creates a generalized linear model (GLM) to output a response between 0 to 1 that represents the probability of a given observation for a binary target variable.\n",
    "\n",
    "<img alt=\"Logistic Regression Curve\" src=\"assets/logistic_regression_curve.png\" width=\"200\">\n",
    "\n",
    "Similarly in linear regression, we can do the following:\n",
    "\n",
    "- Transformations of input data\n",
    "- Interaction terms\n",
    "- Regularization\n",
    "- Trees / random forests\n",
    "\n",
    "But logistic regression takes longer to calculate with no closed-form solution.\n",
    "\n",
    "****\n",
    "\n",
    "## Advanced methods in regression\n",
    "\n",
    "- **Poisson regression**\n",
    "\n",
    "    This can be used when the response follows a Poisson distribution, $f(z) = \\frac{\\lambda^z e^{-\\lambda}}{z!}$, for example, discrete count arrivals at an airport security line where the arrival rate might be a function of time and estimate $\\lambda(x)$\n",
    "    \n",
    "    \n",
    "- **Regression splines**\n",
    "\n",
    "    A spline is a function of polynomials that connect to each other. The following image has 4 different splines fitting to the data:\n",
    "    \n",
    "    <img alt=\"Regression splines\" src=\"assets/regression_splines.png\" width=\"300\">\n",
    "    \n",
    "    This fits different functions to different parts of the dataset to allow for smooth connections between parts. An order-$k$ regression spline is such that the polynomials are of order $k$. One such algorithm is called **MARS** (*multivariate adaptive regression splines*) but is commonly implemented as a code package called [*Earth*](https://github.com/scikit-learn-contrib/py-earth).\n",
    "    \n",
    "    \n",
    "- **Bayesian regression**\n",
    "\n",
    "    Starts with a prior distribution on regression coefficients and computes the posterior distribution given data. Most helpful **when there is not much data** and we can leverage on experts' opinions to define a prior, or use a very broad prior when no such information is available.\n",
    "    \n",
    "    \n",
    "- **$k$-Nearest-Neighbor Regression**\n",
    "\n",
    "    No estimate of prediction function and predict the response by weighted averages (*commonly by distance*) of the $k$ closest data points.\n",
    "    \n",
    "****\n",
    "\n",
    "## Variable Selection\n",
    "\n",
    "Using too many factors in a model can lead to two main problems:\n",
    "\n",
    "1. Overfitting\n",
    "    - When # of factors is close to or larger than # of data points\n",
    "    - The model might fit too closely to random effects\n",
    "    \n",
    "    \n",
    "2. Difficulty of interpretation\n",
    "    - Overly-complex models can be hard to interpret, especially when factors are correlated with each other\n",
    "    \n",
    "    \n",
    "****\n",
    "\n",
    "**Variable selection techniques**\n",
    "\n",
    "1. **Forward/Backwards/Stepwise Regression**\n",
    "\n",
    "    Decisions are made step-by-step and is known as the *Greedy Algorithm*. At each step, take one thing that looks best without consideration of future options.\n",
    "\n",
    "    1. Forward Selection\n",
    "        - Start with no factors\n",
    "        - Evaluate each factor one by one and select factors that have a p-value below a given threshold (say, $p \\leq 0.15$)\n",
    "        - Fit model with given set of signficant factors\n",
    "        - Remove factors with high p-value\n",
    "        - Re-fit model with final set of factors   \n",
    "        \n",
    "        <img alt=\"Forward\" src=\"assets/forward.png\" width=\"300\">\n",
    "    \n",
    "    2. Backwards Elimination\n",
    "        - Start with all factors\n",
    "        - Evaluate factors and find worst factor\n",
    "        - Remove factors above p-threshold (say, $p > 0.15$)\n",
    "        - Fit model with final set of factors\n",
    "        \n",
    "    3. Stepwise (*Hybrid of forward / backswards*)\n",
    "        - Start with no factors\n",
    "        - Evaluate each factor and find best factor (lowest p-value)\n",
    "        - If factor has $p \\leq 0.15$ then add factor into the model\n",
    "        - Fit model with current set of factors\n",
    "        - Remove factors with high p-value (say, $p > 0.15$)\n",
    "        - Fit model with final set of factors and remove factors with a stricter threshold ($p > 0.05$)\n",
    "        \n",
    "        <img alt=\"Stepwise\" src=\"assets/stepwise.png\" width=\"500\">\n",
    "\n",
    "    Alternative metrics such as AIC or BIC can be considered instead of p-value as well.\n",
    "    \n",
    "    \n",
    "2. **Regularized Regression**\n",
    "\n",
    "    Adds a constraint to the standard regresion equation. In standard linear regression, the model aims to minimize the squared error as follows:\n",
    "    \n",
    "    $\\min \\sum_{i=1}^{n} (y_i - \\hat y_i)^2$\n",
    "    \n",
    "    where each $\\hat y_i$ is predicted by $X^T \\beta =$\n",
    "    \n",
    "    In Lasso regression, we add a constaint such that the equation is now as follows:\n",
    "    \n",
    "    $\\min \\sum_{i=1}^{n} (y_i - \\hat y_i)^2 + \\lambda \\sum_{j = 0}^{p} \\vert b_j \\vert$\n",
    "    \n",
    "    where $\\lambda \\sum_{j = 0}^{p} \\vert b_j \\vert$ is a penalty term that penalizes each factor's weight if the weight becomes big. Choosing a low $\\lambda$ will mean that the penalty term diminishes and resembles the original linear regression model without penalizing the weights. **Scaling the data is necessary** for lasso regression to work.\n",
    "    \n",
    "    This method can lead to zero coefficients which means the variable is completely neglected in generating the model's output and therefore helps with variable selection that would offer the best prediction quality.\n",
    "    \n",
    "    A variant of the lasso regression is the *elastic net* which is a combination of the lasso and ridge regression methods such that the equation is now as follows:\n",
    "    \n",
    "    $\\min \\sum_{i=1}^{n} (y_i - \\hat y_i)^2 + \\lambda \\sum_{j = 0}^{p} \\vert b_j \\vert + (1 - \\lambda) \\sum_{j = 0}^{p} b_j^2$\n",
    "    \n",
    "    Can lead to better predictive ability and needs to be evaluated against the different options. \n",
    "    \n",
    "    Generally, the Lasso regression approach simplifies models by forcing coefficients to be zero while ridge regression forces coefficients to shrink to zero and reduces variance in estimation and the elastic net captures the benefits of both variable selection of lasso and predictive benefits of ridge. \n",
    "    \n",
    "    But similarly captures the disadvantages of both where Lasso rules out some correlated variable arbitrarily and ridge damps the coefficients of highly influential predictive variables for lesser variance.\n",
    "\n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
