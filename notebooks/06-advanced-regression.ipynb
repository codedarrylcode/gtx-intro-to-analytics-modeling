{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Regression\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CART\n",
    "*Classification and regression trees*\n",
    "\n",
    "Suppose if a single covariate reflects a realistic situation where mixed effects are observed, for example, data points with age $\\leq 25$ and data points with age $> 25$, then a decision tree can fit a regression model and estimate its relevant coefficients in each set of data points, conditional on age.\n",
    "\n",
    "<img alt=\"CART\" src=\"assets/CART.png\" width=\"500\">\n",
    "\n",
    "This offers two direct benefits:\n",
    "\n",
    "1. Ability to examine each individual leaf's coefficients and explain the covariate's relationship with the target variable\n",
    "\n",
    "\n",
    "2. Higher predictive accuracy with targeted predictions\n",
    "\n",
    "****\n",
    "\n",
    "**Branching**\n",
    "\n",
    "*How does branching work?*\n",
    "\n",
    "In general, the idea is to do the following:\n",
    "\n",
    "- Use half of the data to begin branching\n",
    "- Find the best factor to branch\n",
    "    - Split the data on each factor\n",
    "    - Calculate the [mutual information](https://github.com/codedarrylcode/mitx-statistical-modeling/blob/master/notebooks/05a-gaussian-processes.ipynb) between factor and target\n",
    "    - Find the factor that has the largest mutual information\n",
    "- Accept branch if mutual information is more than threshold\n",
    "- Repeat until no further splits that are more than threshold\n",
    "- Use the other half of the data to prune the tree\n",
    "    - For each branch, calculate estimation error with/without the tree (*did this really improve the model?*)\n",
    "    - If branching increases error, then remove branching\n",
    "    \n",
    "\n",
    "<img alt=\"Branching\" src=\"assets/branching.png\" width=\"500\">\n",
    "\n",
    "In general, over-branching is likely to cause overfitting and a branch should be rejected if it doesn't cross the threshold, i.e. improvement benefit vs the cost of overfitting. A rule of thumb to prevent overfitting is to ensure that the leaf contains at least 5% of the original data\n",
    "\n",
    "****\n",
    "\n",
    "## Random Forests\n",
    "\n",
    "The idea is to introduce randomness and generate many different trees such that the average of these trees outperforms a single tree. This procedure is called **bagging** (*bootstrap aggregating*) and also known as one of the [ensemble learning](https://en.wikipedia.org/wiki/Ensemble_learning) methods.\n",
    "\n",
    "The general procedure is as follows:\n",
    "\n",
    "1. Bootstrapping $n$ data points with replacement from the original dataset\n",
    "\n",
    "2. Randomly choose a small subset of factors for branching, for e.g. $1 + \\log(n)$ # of factors\n",
    "\n",
    "3. No pruning necessary\n",
    "\n",
    "4. For regression, the average of all predictions are used and for classification, the most common predicted response is used (*averages also can be used as probability*)\n",
    "\n",
    "| Benefits      | Drawbacks |\n",
    "| ----------- | ----------- |\n",
    "| Better overall estimates      | Harder to explain/interpret results       |\n",
    "| Averages between trees and somewhat neutralizes over-fitting   | Can't give us a specific regression or classification model from the data        |\n",
    "\n",
    "****\n",
    "\n",
    "## Logistic Regression\n",
    "*Sometimes called a logit model*\n",
    "\n",
    "A standard linear regression model is as follows:\n",
    "\n",
    "$y = a_0 + a_1 x_1 + \\dots + a_j x_j = X^T \\alpha$ where $\\alpha$ is a vector of coefficients as estimated by MLE\n",
    "\n",
    "A logistic regression model transforms the above:\n",
    "\n",
    "$\\log (\\frac{p}{1-p}) = a_0 + a_1 x_1 + \\dots + a_j x_j = X^T \\alpha$\n",
    "\n",
    "$\\therefore p = \\frac{1}{1 + e^{-X^T \\alpha}}$\n",
    "\n",
    "This transformation creates a generalized linear model (GLM) to output a response between 0 to 1 that represents the probability of a given observation for a binary target variable.\n",
    "\n",
    "<img alt=\"Logistic Regression Curve\" src=\"assets/logistic_regression_curve.png\" width=\"200\">\n",
    "\n",
    "Similarly in linear regression, we can do the following:\n",
    "\n",
    "- Transformations of input data\n",
    "- Interaction terms\n",
    "- Regularization\n",
    "- Trees / random forests\n",
    "\n",
    "But logistic regression takes longer to calculate with no closed-form solution.\n",
    "\n",
    "****\n",
    "\n",
    "## Advanced methods in regression\n",
    "\n",
    "- **Poisson regression**\n",
    "\n",
    "    This can be used when the response follows a Poisson distribution, $f(z) = \\frac{\\lambda^z e^{-\\lambda}}{z!}$, for example, discrete count arrivals at an airport security line where the arrival rate might be a function of time and estimate $\\lambda(x)$\n",
    "    \n",
    "    \n",
    "- **Regression splines**\n",
    "\n",
    "    A spline is a function of polynomials that connect to each other. The following image has 4 different splines fitting to the data:\n",
    "    \n",
    "    <img alt=\"Regression splines\" src=\"assets/regression_splines.png\" width=\"300\">\n",
    "    \n",
    "    This fits different functions to different parts of the dataset to allow for smooth connections between parts. An order-$k$ regression spline is such that the polynomials are of order $k$. One such algorithm is called **MARS** (*multivariate adaptive regression splines*) but is commonly implemented as a code package called [*Earth*](https://github.com/scikit-learn-contrib/py-earth).\n",
    "    \n",
    "    \n",
    "- **Bayesian regression**\n",
    "\n",
    "    Starts with a prior distribution on regression coefficients and computes the posterior distribution given data. Most helpful **when there is not much data** and we can leverage on experts' opinions to define a prior, or use a very broad prior when no such information is available.\n",
    "    \n",
    "    \n",
    "- **$k$-Nearest-Neighbor Regression**\n",
    "\n",
    "    No estimate of prediction function and predict the response by weighted averages (*commonly by distance*) of the $k$ closest data points.\n",
    "    \n",
    "****"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
