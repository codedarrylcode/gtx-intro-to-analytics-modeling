{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines (SVM)\n",
    "_Also known as Large Margin Classifiers_ \n",
    "\n",
    "**Theoretical Documentation**<br>\n",
    "\n",
    "Find the maximum-margin hyperplane in an D-dimensional space that **divides** a group of binary points for which $y_i = 1$ and $y_i = -1$ such that the distance between the hyperplane and the nearest point $x_i$ from either group is **maximized**.\n",
    "\n",
    "<img alt=\"Support Vector Machine\" src=\"assets/math-e04fb447.png\" width=\"400\" >\n",
    "\n",
    "### The simple math behind it\n",
    "Find a decision boundary (hyperplane) that is parallel to two support vectors and lies halfway between them. More formally it is defined as the following equation:<br><br>\n",
    "$w^T X - b = 0$\n",
    "\n",
    "where\n",
    "- $X$ is a matrix in $n$ x $d$ dimensions\n",
    "- $w$ is a vector orthogonal to the hyperplane and projects it to the support vector\n",
    "- $b$ is the bias that translates the hyperplane away from the origin (i.e. the intercept)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/7/72/SVM_margin.png/600px-SVM_margin.png\" width=\"300\">\n",
    "\n",
    "\n",
    "Since distance between two hyperplanes is equal to $\\frac{2}{\\lVert w \\rVert}$, âˆ´ minimizing $\\lVert w \\rVert$ will **maximize** the margin between the classes\n",
    "\n",
    "\n",
    "### Evaluating margin classification error\n",
    "_If a given observation is on the wrong side of the plane, then what is the error associated with it?_ <br><br>\n",
    "**Error of a given observation**, *j*:<br>\n",
    "\n",
    "$error_j = max(0, 1 - y_j(w^T X_{j} - b))$<br>\n",
    "\n",
    "**Total error**: $\\sum_{j=1}^{n} error_j$\n",
    "\n",
    "where $y_j$ is the response variable (1 or -1), such that $y_j(w^T X_{j} - b) >= 1$ for both responses\n",
    "\n",
    "### Cost function of a `soft-margin` SVM\n",
    "Contains both margin classification error and margin from hyperplane.\n",
    "\n",
    "$\\displaystyle\\arg \\min_{\\substack{a_0 \\dots a_d}} \\sum_{j=1}^{n} error_j + \\lambda \\lVert w \\rVert ^2$\n",
    "\n",
    "where $\\lambda$ determines the trade-off between increasing the margin size and ensuring that the observations lie on the correct side of the margin, i.e. for small values of $\\lambda$, the second term goes away and the importance of a small error outweighs the importance of a large margin\n",
    "\n",
    "### What exactly is a support vector?\n",
    "> \"If we connect the dots on the outside of the points (the convex hull), support vectors are the points parallel to the shape in a given direction\"\n",
    "\n",
    "<img alt=\"math-1436320c.png\" src=\"assets/math-1436320c.png\" width=\"500\">\n",
    "\n",
    "### Practical tips\n",
    "- Possible to bias decision boundary towards one response (considering its importance) by choosing $b$ adequately, such that $b = w_1 (b_0 - 1) + w_2 (b_0 + 1)$ where $w_1+w_2=1$\n",
    "- Add a penalty term to computing $error_j$ for a given response to emphasize importance on margin error for given response\n",
    "    - $\\displaystyle\\arg \\min_{\\substack{a_0 \\dots a_d}} \\sum_{j=1}^{n} m_j \\cdot error_j + \\lambda \\lVert w \\rVert ^2$\n",
    "    - where $m_j > 1$ for more-costly errors and $m_j < 1$ for less-costly errors\n",
    "- Scaling / standardization is necessary prior to running SVM\n",
    "- Near-zero $w_i$ coefficients suggest that the i-th dimension does not contribute to the classification\n",
    "- Works the same in multi-variate problems\n",
    "- Kernel methods allow non-linear classifiers (doesn't always have to be straight-lines or linear classifiers)\n",
    "\n",
    "```\n",
    "Want to go deeper? For further reading, try http://pyml.sourceforge.net/doc/howto.pdf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic code\n",
    "A `minimal, reproducible example`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-01T15:46:11.724859Z",
     "start_time": "2021-09-01T15:46:11.720579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25.1\n"
     ]
    }
   ],
   "source": [
    "# Some code here\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
